{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Image Classification\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "3. [Model Implementation](#Model-Implementation)\n",
    "3. [Training the CNN model](#Training-the-CNN-model)\n",
    "4. [Evaluation](#Evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to our end-to-end example of image classification algorithm. In this demo, we will use the Keras library for  image classification algorithm to train on the Plural point INC dataset.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps by importing all teh required libraries, loading the images, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "To import all the required libaries for implementing the machine learing algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in the CNN Implementation\n",
    "\n",
    "To train and validate the data CNN is performed in four steps.\n",
    "1.\t**Convolution:** it is performed on the input image using either filters or kernels. For better performance of the convolution the image is scanned from top left to right and covering the width of screen and iterating the process till the entire image is scanned. \n",
    "2.\t**Pooling:** A common approach to address the image is the down sampling process. Post the convolution layer is formed effectively applied, the nonlinear ReLU, the pooling layer is added to feature the mapped output. It involves selecting the pooling factor and reducing the image by this factor. \n",
    "3.\t**Flattening:** In this step all the polled features are flattened into a column for further processing. This is required to make the data ready for being feed into the neural network. \n",
    "4.\t**Full Connection:** the full connection layer takes the inputs from the feature analysis and applies weights to predict the correct label associated and finally the output layer of the full connection give the prediction of each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 6, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 images belonging to 6 classes.\n",
      "Found 17 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('train_final/',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('test_final/',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters specific for the use of algorithm include:\n",
    "\n",
    "1. **training_set**: the set used to train the model \n",
    "2. **steps_per_epoch**: the number of batch iterations before a training epoch is considered finished\n",
    "3. **epochs**: Number of training epochs\n",
    "4. **validation_data**: Learning the data \n",
    "5. **validation_steps**: the functionality is similar to steps_per_epoch, it works on the validation dataset which \n",
    "here is the test dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 371s 742ms/step - loss: 0.0683 - accuracy: 0.9797 - val_loss: 1.1973 - val_accuracy: 0.8824\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 481s 963ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 1.0690 - val_accuracy: 0.9412\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 389s 778ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.9691 - val_accuracy: 0.9412\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 397s 794ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 1.2061 - val_accuracy: 0.9412\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 363s 727ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.7453 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x12e82fc5278>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 500,\n",
    "                         epochs = 5,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set:  0.9411764740943909\n"
     ]
    }
   ],
   "source": [
    "accuracy=classifier.evaluate_generator(test_set,100)\n",
    "print('Accuracy of the model on the test set: ',accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img('iCard_021992_1_DALDORF_ALICE.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0.]]\n",
      "type3\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('iCard_021878_1_DABNEY_ERWIN_OTIS.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "print(result)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    print('type1')\n",
    "elif result[0][1]==1:\n",
    "    print('type2')\n",
    "elif result[0][2]==1:\n",
    "    print('type3')\n",
    "elif result[0][3]==1:\n",
    "    print('type4')scree\n",
    "else:\n",
    "    print('type5')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
